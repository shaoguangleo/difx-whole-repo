#!/usr/bin/env bash
#===========================================================================
# Copyright (C) 2021  Max-Planck-Institut f√ºr Radioastronomie, Bonn, Germany
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.
#===========================================================================
# SVN properties (DO NOT CHANGE)
#
# $Id: difxslurm 10135 2021-09-02 10:11:36Z HelgeRottmann $
# $HeadURL$
# $LastChangedRevision: 10135 $
# $Author: HelgeRottmann $
# $LastChangedDate: 2021-09-02 12:11:36 +0200 (Do, 02. Sep 2021) $
#
#============================================================================

# Shell script to allow starting of heterogeneous DiFX jobs through the SLURM batch scheduler.
# DiFX jobs will be heterogeneous when containing e.g. mark5 and/or mark5 playback units or
# if a specific head node is required.
# The "normal" SLURM mechanism of using jobsteps is unsuited as it will always execute in 
# exclusive mode when using jobsteps which will prevent starting of multiple DiFX jobs in
# parallel.
# Execute with startdifx -A difxslurm ...
#============================================================================

# make sure salloc is started in background
set -m

# make sure slurm job gets canceled on SIGINT
trap cleanExit SIGINT

function cleanExit {
    scancel $JOB_ID
    kill -9 $PID
    echo "done"
    exit
}

path=`dirname $1`
job=`basename $1`

# Specific settings (adjust for your cluster)
partition=correlator
threads=19


cd $path
# generate .machine file (datastreams only)
genmachines --nocompute $job.input
mv $job.machines $job.dsmachines

# parse the .dsmachines file and
# contruct slurm jobsteps
# for all datastream processes
jobsteps=""
mkfifo mypipe
sort $job.dsmachines | uniq -c > mypipe &

while read count node
do
   jobsteps+=" -p $partition --ntasks=$count --nodes=1 --nodelist=$node :"
   #echo    $count $node
done < mypipe
rm mypipe

jobsteps="${jobsteps::-1}"
stepcount=`echo $jobsteps | tr -cd ':' | wc -c | tr -d ' '`
((stepcount++))

# TODO: calculate resource requirements
# Each thread can roughly do 85Mbps
# speedup = scanDuration / correlationDuration   (desirable: 40)
# np = numDS * dataRate * speedup / 85

np=`difxresource -r 45 $job.input | tail -1`
nodes=$(((np+threads-1)/threads))

#echo $np
#echo $jobsteps

echo "Using $nodes compute nodes with $threads threads to process this job"

#write the slurm batch file
#echo "HOSTFILE=$job.machines" > $job.slurm
#echo "cat $job.dsmachines > \$HOSTFILE" >> $job.slurm
#echo "srun --pack-group=$stepcount hostname -s >> \$HOSTFILE" >> $job.slurm
#echo "numtasks=\`cat \$HOSTFILE | wc -l\`" >> $job.slurm
#echo "mpirun -np \$numtasks --hostfile \$HOSTFILE $DIFX_MPIRUNOPTIONS /cluster/difx/runmpifxcorr.$DIFX_VERSION $job.input" >> $job.slurm
#chmod u+x $job.slurm

# obtained list of io nodes (to be excluded from computation)
exclude=`sinfo -s -p correlator --format "%N" | grep -oP 'io.*?]'`

# construct the hostfile
HOSTFILE=$job.machines
cat $job.dsmachines > $HOSTFILE
cnodes=`srun -p correlator --cpus-per-task=$threads --ntasks-per-node=1 --nodes=$nodes --exclude=$exclude,mark6-[01-11],frontend,fxmanager hostname -s`
PID=$!

cnodelist=`echo $cnodes | sed 's/ /,/g'`
cnodecount=`echo $cnodes | wc --words`

echo $cnodes | sed 's/ /\n/g' >> $HOSTFILE
echo "Was assigned the following $cnodecount nodes: $cnodelist"

numtasks=`cat $HOSTFILE | wc -l`
numds=`cat $job.dsmachines | wc -l`

# check that hostfile contains the correct number 
#echo $numtasks
#echo $nodes
#echo $(($numtasks - $nodes))
if [[ $(($numtasks - $nodes)) -ne $numds ]]; then
    echo "The $HOSTFILE is incomplete. Check state of SLURM nodes."
    cleanExit
fi

rm -f /tmp/$job
# make dummy allocation
salloc -J $job $jobsteps : -p correlator --cpus-per-task=$threads --ntasks-per-node=1 --nodelist=$cnodelist &> /tmp/$job &
echo $jobsteps

sleep 1
# get pid of salloc process
PID=$!

JOB_ID=""
# parse salloc output to see whether resources have been granted
# salloc: job 10912 queued and waiting for resources
regGrant='salloc: Granted job allocation ([0-9]+)'
regQueue='salloc: job ([0-9]+) queued'
regFail='salloc: error'
START=0
echo "Waiting for SLURM resource allocation"
while [ $START -eq 0 ]
do
    while read l; do
        # allocation granted
        if [[ $l =~ $regGrant ]]; then
            #JOB_ID=${BASH_REMATCH[1]}
            START=1
            break
        fi
        #alocation queued
        if [[ $l =~ $regQueue ]]; then
            JOB_ID=${BASH_REMATCH[1]}
            echo $JOB_ID > $job.slurmid
        fi 
        if [[ $l =~ $regFail ]]; then
            echo $l
            cleanExit
        fi
    done < /tmp/$job
done

echo "Granted job allocation" $JOB_ID
mpirun -np $numtasks --hostfile $HOSTFILE $DIFX_MPIRUNOPTIONS /cluster/difx/runmpifxcorr.$DIFX_VERSION $job.input

cleanExit

